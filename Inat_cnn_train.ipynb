{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inat_cnn_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paddy3696/CNN_Pytorch/blob/main/Inat_cnn_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYcsn9UVpEyH",
        "outputId": "1ec18138-c5df-4b74-d3fe-f32ed2c647f5"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Apr 17 20:01:02 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsLHtUM6pJlg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woHcNgnkpLpQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b38364a7-7803-4bd3-ba20-1a5cc63643a3"
      },
      "source": [
        "%pip install wandb -q\n",
        "\n",
        "# Ignore excessive warnings\n",
        "import logging\n",
        "logging.propagate = False \n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "# WandB – Import the wandb library\n",
        "import wandb\n",
        "#wandb.init(project='mnist_classification', entity='paddy3696')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1MB 13.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 51.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 12.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 52.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lVpi2dppObF",
        "outputId": "66b388c1-c86d-4c30-da04-b469a898e5c2"
      },
      "source": [
        "train_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_gpu:\n",
        "    print('CUDA is not available. Training on CPU')\n",
        "else:\n",
        "    print('CUDA is available! Training on GPU')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available! Training on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENHB62dRqM2W",
        "outputId": "58e5c0f4-9b14-47e1-e306-989aa4f662b3"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=4bd717d72ddc962b3c59c7409bd683d0b7ea85ed4e83aa2c37c9a3b8ac4ba48c\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUI8eRZepQex"
      },
      "source": [
        "import wget\n",
        "wget.download('https://storage.googleapis.com/wandb_datasets/nature_12K.zip')\n",
        "!unzip /content/nature_12K.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy7mdYCKpTyU"
      },
      "source": [
        "class MyNN(nn.Module):\n",
        "  def __init__(self, drop_out = 0.2, batch_norm = 'Yes', filter_n = 32,  filter_org = 'same',hidden_out = 128):\n",
        "\n",
        "    super(MyNN,self).__init__()\n",
        "    \n",
        "    self.drop_out = drop_out\n",
        "    self.batch_norm = batch_norm\n",
        "    self.filter_n = filter_n\n",
        "    self.filter_org = filter_org\n",
        "    self.hidden_out = hidden_out\n",
        "    \n",
        "    if self.filter_org == 'same':\n",
        "      self.conv_1 = nn.Conv2d(3,self.filter_n,kernel_size=3,padding=1)\n",
        "      self.conv_1_bn = nn.BatchNorm2d(self.filter_n)\n",
        "      self.conv_2 = nn.Conv2d(self.filter_n,self.filter_n,kernel_size=3,padding=1)\n",
        "      self.conv_2_bn = nn.BatchNorm2d(self.filter_n)\n",
        "      self.conv_3 = nn.Conv2d(self.filter_n,self.filter_n,kernel_size=3,padding=1)\n",
        "      self.conv_3_bn = nn.BatchNorm2d(self.filter_n)\n",
        "      self.conv_4 = nn.Conv2d(self.filter_n,self.filter_n,kernel_size=3,padding=1)\n",
        "      self.conv_4_bn = nn.BatchNorm2d(self.filter_n)\n",
        "      self.conv_5 = nn.Conv2d(self.filter_n,self.filter_n,kernel_size=3,padding=1)\n",
        "      self.conv_5_bn = nn.BatchNorm2d(self.filter_n)\n",
        "\n",
        "      self.fc1 = nn.Linear(self.filter_n*7*7,self.hidden_out)\n",
        "      self.fc2 = nn.Linear(self.hidden_out,10)\n",
        "\n",
        "    if self.filter_org == 'double_up':\n",
        "      self.conv_1 = nn.Conv2d(3,self.filter_n,kernel_size=3,padding=1)\n",
        "      self.conv_1_bn = nn.BatchNorm2d(self.filter_n)\n",
        "      self.conv_2 = nn.Conv2d(self.filter_n,self.filter_n*2,kernel_size=3,padding=1)\n",
        "      self.conv_2_bn = nn.BatchNorm2d(self.filter_n*2)\n",
        "      self.conv_3 = nn.Conv2d(self.filter_n*2,self.filter_n*4,kernel_size=3,padding=1)\n",
        "      self.conv_3_bn = nn.BatchNorm2d(self.filter_n*4)\n",
        "      self.conv_4 = nn.Conv2d(self.filter_n*4,self.filter_n*8,kernel_size=3,padding=1)\n",
        "      self.conv_4_bn = nn.BatchNorm2d(self.filter_n*8)\n",
        "      self.conv_5 = nn.Conv2d(self.filter_n*8,self.filter_n*16,kernel_size=3,padding=1)\n",
        "      self.conv_5_bn = nn.BatchNorm2d(self.filter_n*16)\n",
        "\n",
        "      self.fc1 = nn.Linear(self.filter_n*16*7*7,self.hidden_out)\n",
        "      self.fc2 = nn.Linear(self.hidden_out,10)\n",
        "\n",
        "    if self.filter_org == 'double_down':\n",
        "      self.conv_1 = nn.Conv2d(3,self.filter_n,kernel_size=3,padding=1)\n",
        "      self.conv_1_bn = nn.BatchNorm2d(self.filter_n)\n",
        "      self.conv_2 = nn.Conv2d(self.filter_n, int(self.filter_n/2),kernel_size=3,padding=1)\n",
        "      self.conv_2_bn = nn.BatchNorm2d(int(self.filter_n/2))\n",
        "      self.conv_3 = nn.Conv2d(int(self.filter_n/2), int(self.filter_n/4),kernel_size=3,padding=1)\n",
        "      self.conv_3_bn = nn.BatchNorm2d(int(self.filter_n/4))\n",
        "      self.conv_4 = nn.Conv2d(int(self.filter_n/4), int(self.filter_n/8),kernel_size=3,padding=1)\n",
        "      self.conv_4_bn = nn.BatchNorm2d(int(self.filter_n/8))\n",
        "      self.conv_5 = nn.Conv2d(int(self.filter_n/8), int(self.filter_n/16),kernel_size=3,padding=1)\n",
        "      self.conv_5_bn = nn.BatchNorm2d(int(self.filter_n/16))\n",
        "\n",
        "      self.fc1 = nn.Linear(int(self.filter_n/16)*7*7,self.hidden_out)\n",
        "      self.fc2 = nn.Linear(self.hidden_out,10)\n",
        "\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.dropout = nn.Dropout(self.drop_out)\n",
        "\n",
        "  def forward(self,x):\n",
        "    if self.batch_norm == 'Yes':\n",
        "      x = self.pool(F.relu(self.conv_1_bn(self.conv_1(x))))\n",
        "      x = self.pool(F.relu(self.conv_2_bn(self.conv_2(x))))\n",
        "      x = self.pool(F.relu(self.conv_3_bn(self.conv_3(x))))\n",
        "      x = self.pool(F.relu(self.conv_4_bn(self.conv_4(x))))\n",
        "      x = self.pool(F.relu(self.conv_5_bn(self.conv_5(x))))\n",
        "      if self.filter_org == 'same':\n",
        "        x = x.view(-1,self.filter_n*7*7)\n",
        "      elif self.filter_org == 'double_up':\n",
        "        x = x.view(-1,self.filter_n*16*7*7)\n",
        "      elif self.filter_org == 'double_down':\n",
        "        x = x.view(-1,int(self.filter_n/16)*7*7)\n",
        "      x = self.dropout(x)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "\n",
        "    if self.batch_norm == 'No':\n",
        "      x = self.pool(F.relu(self.conv_1(x)))\n",
        "      x = self.pool(F.relu(self.conv_2(x)))\n",
        "      x = self.pool(F.relu(self.conv_3(x)))\n",
        "      x = self.pool(F.relu(self.conv_4(x)))\n",
        "      x = self.pool(F.relu(self.conv_5(x)))\n",
        "      if self.filter_org == 'same':\n",
        "        x = x.view(-1,self.filter_n*7*7)\n",
        "      elif self.filter_org == 'double_up':\n",
        "        x = x.view(-1,self.filter_n*16*7*7)\n",
        "      elif self.filter_org == 'double_down':\n",
        "        x = x.view(-1,int(self.filter_n/16)*7*7)\n",
        "      x = self.dropout(x)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "\n",
        "    #return F.log_softmax(x,dim= 1)\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUrKP2BApXJu"
      },
      "source": [
        "def fit(model,train_loader,val_loader,optimizer,epoch,criterion):\n",
        "  acc_epoch = []\n",
        "\n",
        "  for epoch in range(1, epoch+1):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    print('training started')\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "      if train_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()*data.size(0)\n",
        "\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    print('Training over')\n",
        "    model.eval()\n",
        "    for data, target in val_loader:\n",
        "      batch_data_size = data.size(0)\n",
        "      if train_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      val_loss += loss.item()*data.size(0)\n",
        "\n",
        "      _, pred = torch.max(output, 1)    \n",
        "      correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "      correct = np.squeeze(correct_tensor.numpy()) if not train_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "\n",
        "      for i in range(batch_data_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "    \n",
        "    val_loss = val_loss/len(val_loader.sampler)\n",
        "    val_accuracy = 100. * np.sum(class_correct) / np.sum(class_total)\n",
        "    acc_epoch.append(val_accuracy)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, val_loss))\n",
        "    print('Val Accuracy (Overall): %.2f%% (%2d/%2d)\\n' % (val_accuracy,np.sum(class_correct), np.sum(class_total)))\n",
        "\n",
        "    wandb.log({\"Train Loss\": train_loss,\"Val Loss\": val_loss,\"Val Accuracy\": val_accuracy})\n",
        "  \n",
        "  acc_best = max(acc_epoch)\n",
        "  wandb.log({'accuracy' : acc_best})"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4xJBaL_pZq1"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'drop_out': {\n",
        "            'values': [0.2, 0.3]\n",
        "        },\n",
        "        'batch_norm': {\n",
        "            'values': ['Yes', 'No']\n",
        "        },\n",
        "        'filter_n': {\n",
        "            'values': [64, 128]\n",
        "        \n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32]\n",
        "        },\n",
        "        'filter_org': {\n",
        "            'values': ['same', 'double_up', 'double_down']\n",
        "        },\n",
        "        'epoch': {\n",
        "            'values': [5,10]\n",
        "        },\n",
        "        'data_aug': {\n",
        "            'values': ['Yes', 'No']\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['SGD','ADAM'] \n",
        "        },\n",
        "        'lr': {\n",
        "            'values': [0.1,0.01] \n",
        "        },\n",
        "        'hidden_out': {\n",
        "            'values': [128,196] \n",
        "        },\n",
        "    }\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llLFio2WpbbO"
      },
      "source": [
        "#sweep_id = wandb.sweep(sweep_config, entity=#####, project=\"cnn_inat\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8v9Btq_pdGM"
      },
      "source": [
        "def train():\n",
        "  # Default values for hyper-parameters we're going to sweep over\n",
        "  config_defaults = {\n",
        "      'epoch': 10,\n",
        "      'batch_norm': 'Yes',\n",
        "      'filter_n': 64,\n",
        "      'filter_org': 'same',\n",
        "      'data_aug': 'Yes',\n",
        "      'optimizer': 'SGD',\n",
        "      'batch_size': 16,\n",
        "      'seed': 9,\n",
        "      'lr': 9,\n",
        "      'hidden_out':128,\n",
        "      'drop_out': 0.2,\n",
        "  }\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  wandb.init(config=config_defaults)\n",
        "   \n",
        "  # Config is a variable that holds and saves hyperparameters and inputs\n",
        "  config = wandb.config\n",
        "\n",
        "  data_dir = '/content/inaturalist_12K'\n",
        "                              \n",
        "  if config.data_aug == 'Yes':\n",
        "    train_transforms = transforms.Compose([transforms.RandomRotation(30),transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),transforms.ToTensor(), \n",
        "                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "                                           \n",
        "  elif config.data_aug == 'No':\n",
        "    train_transforms = transforms.Compose([transforms.RandomResizedCrop(224), transforms.ToTensor()])\n",
        "\n",
        "  train_data = datasets.ImageFolder(data_dir + '/train',transform=train_transforms)\n",
        "\n",
        "  batch_size = config.batch_size\n",
        "  validation_split = .1\n",
        "  shuffle_dataset = True\n",
        "  random_seed= config.seed\n",
        "\n",
        "  dataset_size = len(train_data)\n",
        "  indices = list(range(dataset_size))\n",
        "  split = int(np.floor(validation_split * dataset_size))\n",
        "  if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "  train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "  train_sampler = SubsetRandomSampler(train_indices)\n",
        "  valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,sampler=train_sampler)\n",
        "  val_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,sampler=valid_sampler)\n",
        "  print('data ready')\n",
        "  \n",
        "  model = MyNN(drop_out = config.drop_out, batch_norm = config.batch_norm, filter_n = config.filter_n,  filter_org = config.filter_org, hidden_out =config.hidden_out)\n",
        "  print(model)\n",
        "\n",
        "  if train_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  #criterion = nn.NLLLoss()\n",
        "  epoch = config.epoch\n",
        "\n",
        "  if config.optimizer == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config.lr,momentum=0.1)\n",
        "  elif config.optimizer == 'ADAM':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "  wandb.run.name = str(config.filter_n) +'_'+ str(config.filter_org) + '_bs_' + str(config.batch_size)\n",
        "\n",
        "  fit(model,train_loader,val_loader,optimizer,epoch,criterion)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "-p2hfQJmpd9R",
        "outputId": "866c7fdc-c754-4683-acb2-e74b377feb9f"
      },
      "source": [
        "#wandb.agent(sweep_id, train,count=100)\n",
        "wandb.agent(\"22yfvs8p\", entity=\"paddy3696\",project=\"cnn_inat\", function =train,count=100)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: roezs950 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: Yes\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_aug: No\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_out: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_n: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: double_down\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_out: 196\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaddy3696\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.26<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">daily-sweep-112</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/paddy3696/cnn_inat\" target=\"_blank\">https://wandb.ai/paddy3696/cnn_inat</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/paddy3696/cnn_inat/sweeps/22yfvs8p\" target=\"_blank\">https://wandb.ai/paddy3696/cnn_inat/sweeps/22yfvs8p</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/paddy3696/cnn_inat/runs/roezs950\" target=\"_blank\">https://wandb.ai/paddy3696/cnn_inat/runs/roezs950</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210417_200255-roezs950</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "data ready\n",
            "MyNN(\n",
            "  (conv_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_2_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_3): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_3_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_4): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_4_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_5): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_5_bn): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=196, out_features=196, bias=True)\n",
            "  (fc2): Linear(in_features=196, out_features=10, bias=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "training started\n",
            "Training over\n",
            "Epoch: 1 \tTraining Loss: 2.274663 \tValidation Loss: 2.215738\n",
            "Val Accuracy (Overall): 18.42% (184/999)\n",
            "\n",
            "training started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 192<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}